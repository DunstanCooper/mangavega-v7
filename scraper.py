#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MangaVega Tracker - Scraper Amazon (HTTP, extraction HTML)
"""

import asyncio
import random
import re
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup

import config
from utils import (
    extraire_asin, extraire_numero_tome, extraire_editeur,
    convertir_editeur_romaji, est_format_papier,
    normaliser_titre, strip_type_suffix
)

logger = config.logger

# Importer curl_cffi si disponible
if config.CURL_CFFI_DISPONIBLE:
    from curl_cffi.requests import AsyncSession as CurlAsyncSession


async def get_html(session, url: str, delai: float = 0.6, max_retries: int = 2) -> Optional[str]:
    """R√©cup√®re le HTML d'une URL Amazon avec anti-d√©tection."""
    est_recherche = '/s?' in url or '/s/' in url
    est_produit = '/dp/' in url
    url_courte = url.split('?')[0][-60:] if '?' in url else url[-60:]
    
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                wait = min(10 * (2 ** attempt) + random.uniform(0, 5), 60)
                logger.info(f"      ‚è≥ Backoff retry #{attempt}: {wait:.0f}s...")
                await asyncio.sleep(wait)
            elif est_recherche:
                wait = random.uniform(2.0, 4.5)
                await asyncio.sleep(wait)
            elif est_produit:
                wait = random.uniform(0.8, 2.0)
                await asyncio.sleep(wait)
            else:
                wait = random.uniform(delai * 0.5, delai * 1.5)
                await asyncio.sleep(wait)
            
            if config.CURL_CFFI_DISPONIBLE and hasattr(session, '_curl_cffi_session'):
                cffi_session = session._curl_cffi_session
                extra_headers = {"Accept-Language": "ja-JP,ja;q=0.9"}
                if est_produit:
                    extra_headers["Referer"] = "https://www.amazon.co.jp/"
                
                response = await cffi_session.get(url, headers=extra_headers, timeout=30, allow_redirects=True)
                status = response.status_code
                content_len = len(response.text) if response.text else 0
                cookies_count = len(cffi_session.cookies) if hasattr(cffi_session, 'cookies') else -1
                logger.info(f"      [HTTP] {status} | {content_len:,} chars | cookies:{cookies_count} | {url_courte}")
                
                if status == 200:
                    html = response.text
                    if html and len(html) < 5000:
                        html_lower = html.lower()
                        if 'captcha' in html_lower or 'robot' in html_lower or 'automated access' in html_lower:
                            logger.warning(f"      ‚ö†Ô∏è  Captcha/bot d√©tect√© dans r√©ponse 200 ({content_len} chars)")
                            continue
                    if html and content_len > 500:
                        return html
                    else:
                        logger.warning(f"      ‚ö†Ô∏è  R√©ponse trop courte ({content_len} chars)")
                        continue
                elif status == 503:
                    logger.warning(f"      ‚ö†Ô∏è  Rate limit (503)")
                    continue
                elif status == 404:
                    logger.info(f"      ‚ÑπÔ∏è  Page introuvable (404)")
                    return None
                else:
                    logger.warning(f"      ‚ö†Ô∏è  HTTP {status}")
                    continue
            else:
                aio_session = session._aiohttp_session if hasattr(session, '_aiohttp_session') else session
                async with aio_session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    content = await response.text()
                    content_len = len(content) if content else 0
                    logger.info(f"      [HTTP-aio] {response.status} | {content_len:,} chars | {url_courte}")
                    if response.status == 200:
                        return content
                    elif response.status == 503:
                        logger.warning(f"      ‚ö†Ô∏è  Rate limit (503), attente 10s...")
                        await asyncio.sleep(10)
                        continue
                    else:
                        logger.warning(f"      ‚ö†Ô∏è  HTTP {response.status}")
                        return None
                    
        except asyncio.TimeoutError:
            logger.warning(f"      ‚ö†Ô∏è  Timeout (attempt {attempt+1}/{max_retries+1})")
            if attempt >= max_retries:
                logger.error(f"      ‚ùå Timeout d√©finitif")
        except Exception as e:
            err_str = str(e)[:80]
            logger.warning(f"      ‚ö†Ô∏è  Erreur (attempt {attempt+1}/{max_retries+1}): {err_str}")
            if attempt >= max_retries:
                logger.error(f"      ‚ùå Erreur d√©finitive: {err_str}")
    return None


class SessionWrapper:
    """Encapsule curl_cffi AsyncSession pour le scraping Amazon."""
    def __init__(self):
        self._curl_cffi_session = None
        self._aiohttp_session = None
        self._warmed_up = False
    
    async def __aenter__(self):
        if config.CURL_CFFI_DISPONIBLE:
            self._curl_cffi_session = CurlAsyncSession(impersonate="chrome", timeout=30)
            await self._curl_cffi_session.__aenter__()
            logger.info("üåê Session HTTP: curl_cffi (impersonate=chrome, TLS+HTTP/2 fingerprint)")
            logger.info("   üìã Headers: Accept-Language: ja-JP (force pages japonaises)")
        else:
            self._aiohttp_session = aiohttp.ClientSession(headers=config.HEADERS)
            await self._aiohttp_session.__aenter__()
            logger.warning("üåê Session HTTP: aiohttp (‚ö†Ô∏è PAS de TLS impersonation)")
            logger.warning("   ‚ö†Ô∏è  curl_cffi non disponible - installer avec: pip install curl-cffi")
        return self
    
    async def __aexit__(self, *args):
        if self._curl_cffi_session:
            await self._curl_cffi_session.__aexit__(*args)
        if self._aiohttp_session:
            await self._aiohttp_session.__aexit__(*args)
    
    async def warm_up(self):
        """Visite amazon.co.jp pour initialiser les cookies de session."""
        if self._warmed_up:
            return
        try:
            logger.info("   üî• Warm-up: visite amazon.co.jp pour recevoir les cookies...")
            if self._curl_cffi_session:
                response = await self._curl_cffi_session.get("https://www.amazon.co.jp/", timeout=15, allow_redirects=True)
                status = response.status_code
                
                # Forcer la langue japonaise via le cookie i18n-prefs
                # Amazon utilise ce cookie pour la devise et la langue de la page
                try:
                    self._curl_cffi_session.cookies.set("i18n-prefs", "JPY", domain=".amazon.co.jp")
                    logger.info("   üáØüáµ Cookie i18n-prefs=JPY inject√© (force pages japonaises)")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è  Impossible d'injecter le cookie i18n-prefs: {e}")
                
                if hasattr(self._curl_cffi_session, 'cookies'):
                    cookies = self._curl_cffi_session.cookies
                    cookie_names = []
                    try:
                        for c in cookies:
                            if hasattr(c, 'name'):
                                cookie_names.append(c.name)
                            elif isinstance(c, str):
                                cookie_names.append(c)
                    except TypeError:
                        try:
                            cookie_names = list(cookies.keys()) if hasattr(cookies, 'keys') else []
                        except:
                            cookie_names = []
                    logger.info(f"   ‚úÖ Warm-up: HTTP {status} | {len(cookie_names)} cookies: {', '.join(cookie_names[:8])}")
                else:
                    logger.info(f"   ‚úÖ Warm-up: HTTP {status}")
            elif self._aiohttp_session:
                async with self._aiohttp_session.get("https://www.amazon.co.jp/", timeout=aiohttp.ClientTimeout(total=15)) as response:
                    logger.info(f"   ‚úÖ Warm-up: HTTP {response.status}")
            self._warmed_up = True
            pause = random.uniform(2.0, 4.0)
            logger.info(f"   ‚è∏Ô∏è  Pause post-warm-up: {pause:.1f}s")
            await asyncio.sleep(pause)
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Warm-up √©chou√©: {str(e)[:80]}")
            logger.warning(f"   ‚Üí On continue quand m√™me (le scan fonctionnera, mais risque accru de 503)")
            self._warmed_up = True


async def extraire_version_papier(html: str, format_cible: str = None, debug: bool = False) -> Optional[str]:
    """Extrait le lien vers la version papier depuis une page Kindle
    
    Args:
        html: HTML de la page Kindle
        format_cible: Type de format √† chercher
            - "manga" ou None : cherche „Ç≥„Éü„ÉÉ„ÇØ/Comic (d√©faut)
            - "ln" : cherche ÊñáÂ∫´/Bunko (light novel)
            - "all" : cherche tous les formats papier
        debug: Afficher les logs de debug
    """
    if not html:
        return None
    
    soup = BeautifulSoup(html, 'lxml')
    
    # D√©finir les keywords selon le format cible
    if format_cible == "ln":
        # Light Novel : chercher Bunko
        papier_keywords = ['ÊñáÂ∫´', 'Bunko']
    elif format_cible == "all":
        # Tous les formats papier
        papier_keywords = ['„Ç≥„Éü„ÉÉ„ÇØ', 'Comic', 'ÊñáÂ∫´', 'Bunko', 'ÂçòË°åÊú¨', 'Tankobon', '„Éö„Éº„Éë„Éº„Éê„ÉÉ„ÇØ', 'Paperback']
    else:
        # Par d√©faut : Manga uniquement
        papier_keywords = ['„Ç≥„Éü„ÉÉ„ÇØ', 'Comic']
    
    kindle_keywords = ['kindle', 'Kindle', '„Éá„Ç∏„Çø„É´', 'ÈõªÂ≠ê']
    
    # Pattern pour extraire un ASIN depuis diff√©rents formats d'URL Amazon
    asin_patterns = [
        r'/dp/([A-Z0-9]{10})',           # /dp/ASIN (classique)
        r'/gp/product/([A-Z0-9]{10})',   # /gp/product/ASIN (ancien format)
        r'/product/([A-Z0-9]{10})',      # /product/ASIN
    ]
    
    def extraire_asin_from_href(href: str) -> Optional[str]:
        """Extrait un ASIN depuis un href avec plusieurs patterns"""
        for pattern in asin_patterns:
            match = re.search(pattern, href)
            if match:
                return match.group(1)
        return None
    
    def extraire_asin_from_element(element) -> Optional[str]:
        """Extrait un ASIN depuis un √©l√©ment HTML (href, data-asin, data-value)"""
        # 1. Chercher dans le href
        href = element.get('href', '')
        asin = extraire_asin_from_href(href)
        if asin:
            return asin
        
        # 2. Chercher dans les attributs data-* de l'√©l√©ment lui-m√™me
        for attr in ['data-asin', 'data-value', 'data-dp-url']:
            val = element.get(attr, '')
            if val:
                asin = extraire_asin_from_href(val)
                if asin:
                    return asin
                # data-asin peut contenir directement l'ASIN sans URL
                if re.match(r'^[A-Z0-9]{10}$', val):
                    return val
        
        # 3. Chercher dans les attributs data-* du parent (li, span, div)
        parent = element.parent
        if parent:
            for attr in ['data-asin', 'data-value', 'data-dp-url']:
                val = parent.get(attr, '')
                if val:
                    asin = extraire_asin_from_href(val)
                    if asin:
                        return asin
                    if re.match(r'^[A-Z0-9]{10}$', val):
                        return val
        
        return None
    
    # Chercher dans la section des formats (tmmSwatches ou MediaMatrix)
    formats_section = soup.find("div", {"id": "tmmSwatches"}) or soup.find("div", {"id": "MediaMatrix"})
    
    if debug:
        logger.info(f"      [DEBUG] formats_section trouv√©e: {formats_section is not None}")
    
    if formats_section:
        for link in formats_section.find_all("a"):
            text = link.get_text().strip()
            href = link.get('href', '')
            
            is_papier = any(kw in text for kw in papier_keywords)
            is_kindle = any(kw in text for kw in kindle_keywords)
            
            if debug:
                logger.info(f"      [DEBUG] Link: '{text[:30]}' papier={is_papier} kindle={is_kindle} href={href[:50]}")
            
            if is_papier and not is_kindle:
                asin_papier = extraire_asin_from_element(link)
                if asin_papier:
                    if debug:
                        logger.info(f"      [DEBUG] ASIN extrait: {asin_papier}")
                    return f"https://www.amazon.co.jp/dp/{asin_papier}"
    
    # Alternative: chercher les swatches individuels
    swatches = soup.find_all("li", class_=lambda x: x and 'swatchElement' in x)
    
    if debug:
        logger.info(f"      [DEBUG] Swatches trouv√©s: {len(swatches)}")
    
    for swatch in swatches:
        text = swatch.get_text().strip()
        link = swatch.find("a", href=True)
        
        if link:
            is_papier = any(kw in text for kw in papier_keywords)
            is_kindle = any(kw in text for kw in kindle_keywords)
            
            if is_papier and not is_kindle:
                asin_papier = extraire_asin_from_element(link)
                # Aussi chercher dans le swatch (li) directement
                if not asin_papier:
                    asin_papier = extraire_asin_from_element(swatch)
                if asin_papier:
                    return f"https://www.amazon.co.jp/dp/{asin_papier}"
    
    return None


async def extraire_volumes_depuis_page(session: aiohttp.ClientSession, url_ou_asin: str, nom_manga: str, 
                                       debug=False, sources: List[str] = None) -> Dict[str, List[str]]:
    """
    Extrait les volumes li√©s depuis une page Amazon.
    
    Sources disponibles (dans l'ordre de fiabilit√©):
    1. "bulk" - Bulk purchases (Êñ∞ÂìÅ„Åæ„Å®„ÇÅË≤∑„ÅÑ) - Le plus fiable
    2. "publisher" - From the Publisher (Âá∫ÁâàÁ§æ„Çà„Çä) - Assez fiable
    
    NOTE: Les carrousels "Frequently bought together" et "Customers also bought" ont √©t√©
    supprim√©s car ils retournaient trop de hors-sujets (mangas "similaires" sans rapport).
    
    Args:
        sources: Liste des sources √† chercher. Si None, cherche ["bulk", "publisher"].
                 
    Returns:
        Dict avec les ASINs par source: {"bulk": [...], "publisher": [...]}
    """
    result = {"bulk": [], "publisher": []}
    
    if sources is None:
        sources = ["bulk", "publisher"]
    
    # Extraire l'ASIN si c'est une URL
    if 'amazon.co.jp' in url_ou_asin:
        match = re.search(r'/dp/([A-Z0-9]{10})', url_ou_asin)
        if match:
            asin = match.group(1)
        else:
            logger.warning(f"      ‚ö†Ô∏è Impossible d'extraire l'ASIN de: {url_ou_asin}")
            return result
    else:
        asin = url_ou_asin
    
    url_produit = f"https://www.amazon.co.jp/dp/{asin}"
    logger.info(f"      üîç Recherche volumes li√©s depuis [{asin}]...")
    
    html = await get_html(session, url_produit)
    if not html:
        return result
    
    # DEBUG: Sauvegarder le HTML pour analyse
    if debug:
        with open(f'debug_page_{asin}.html', 'w', encoding='utf-8') as f:
            f.write(html)
        logger.info(f"      üîç HTML sauvegard√©: debug_page_{asin}.html")
    
    soup = BeautifulSoup(html, 'lxml')
    asins_trouves = set()
    asins_trouves.add(asin)  # Ajouter l'ASIN source pour ne pas le re-traiter
    
    # ========================================
    # SECTION 1: Bulk purchases (Êñ∞ÂìÅ„Åæ„Å®„ÇÅË≤∑„ÅÑ)
    # Le plus fiable - contient uniquement les volumes de la s√©rie
    # ========================================
    if "bulk" in sources:
        bulk_asins = []
        bulk_tomes = {}  # {asin: tome_num} ‚Äî mapping tome depuis le label Bulk
        
        # M√©thode 1: pbnx-desktop-box avec titre du manga
        bulk_boxes = soup.find_all("div", class_="pbnx-desktop-box")
        for box in bulk_boxes:
            titre_span = box.find("span", class_="a-size-base")
            if titre_span:
                titre_texte = titre_span.get_text(strip=True)
                nom_clean = strip_type_suffix(nom_manga); titre_cle = nom_clean[:8] if len(nom_clean) >= 8 else nom_clean
                if normaliser_titre(titre_cle) in normaliser_titre(titre_texte):
                    # Extraire chaque item du Bulk avec son label de tome
                    items = box.find_all("div", class_="pbnx-single-product")
                    if not items:
                        items = box.find_all("li")
                    for item in items:
                        link = item.find("a", href=True)
                        if not link:
                            continue
                        match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                        if match and match.group(1) not in asins_trouves:
                            vol_asin = match.group(1)
                            bulk_asins.append(vol_asin)
                            asins_trouves.add(vol_asin)
                            # Extraire le label de tome (ex: "1Â∑ª", "Vol. 1")
                            label = item.get_text()
                            tome_match = re.search(r'(?:Vol\.?\s*|Á¨¨?\s*)(\d+)\s*Â∑ª?|(\d+)\s*Â∑ª', label)
                            if tome_match:
                                tome_num = int(tome_match.group(1) or tome_match.group(2))
                                bulk_tomes[vol_asin] = tome_num
                    # Fallback : si items non trouv√©s, extraire les liens bruts (ancien code)
                    if not bulk_asins:
                        for link in box.find_all("a", href=True):
                            match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                            if match and match.group(1) not in asins_trouves:
                                bulk_asins.append(match.group(1))
                                asins_trouves.add(match.group(1))
                    break
        
        # M√©thode 2: Header "Bulk purchases" ou "Êñ∞ÂìÅ„Åæ„Å®„ÇÅË≤∑„ÅÑ"
        if not bulk_asins:
            bulk_header = soup.find(lambda tag: tag.name in ['h2', 'h3', 'div', 'span'] and 
                                    ('Bulk purchases' in tag.get_text() or 'Êñ∞ÂìÅ„Åæ„Å®„ÇÅË≤∑„ÅÑ' in tag.get_text()))
            if bulk_header:
                parent = bulk_header.find_parent('div', class_=lambda x: x and 'a-section' in x) or bulk_header.find_parent('div')
                if parent:
                    # Essayer d'extraire avec labels de tome
                    items = parent.find_all("div", class_="pbnx-single-product")
                    if not items:
                        items = parent.find_all("li")
                    for item in items:
                        link = item.find("a", href=True)
                        if link:
                            match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                            if match and match.group(1) not in asins_trouves:
                                vol_asin = match.group(1)
                                bulk_asins.append(vol_asin)
                                asins_trouves.add(vol_asin)
                                label = item.get_text()
                                tome_match = re.search(r'(?:Vol\.?\s*|Á¨¨?\s*)(\d+)\s*Â∑ª?|(\d+)\s*Â∑ª', label)
                                if tome_match:
                                    tome_num = int(tome_match.group(1) or tome_match.group(2))
                                    bulk_tomes[vol_asin] = tome_num
                    # Fallback liens bruts
                    if not bulk_asins:
                        for link in parent.find_all("a", href=True):
                            match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                            if match and match.group(1) not in asins_trouves:
                                bulk_asins.append(match.group(1))
                                asins_trouves.add(match.group(1))
        
        if bulk_asins:
            if bulk_tomes:
                logger.info(f"      üì¶ Bulk: {len(bulk_asins)} volume(s) trouv√©(s) ({len(bulk_tomes)} tome(s) identifi√©(s))")
            else:
                logger.info(f"      üì¶ Bulk: {len(bulk_asins)} volume(s) trouv√©(s)")
        
        result["bulk"] = bulk_asins
        result["bulk_tomes"] = bulk_tomes
    
    # Si le Bulk a trouv√© des r√©sultats, pas besoin des sources moins fiables
    if result["bulk"]:
        return result
    
    # ========================================
    # SECTION 2: From the Publisher / Âá∫ÁâàÁ§æ„Çà„Çä
    # Fallback si Bulk absent ‚Äî m√™me √©diteur
    # ========================================
    if "publisher" in sources:
        publisher_asins = []
        
        # Chercher la section "From the Publisher" ou "Âá∫ÁâàÁ§æ„Çà„Çä"
        publisher_header = soup.find(lambda tag: tag.name in ['h2', 'h3', 'div', 'span'] and 
                                      ('From the Publisher' in tag.get_text() or 
                                       'Âá∫ÁâàÁ§æ„Çà„Çä' in tag.get_text() or
                                       'Products related' in tag.get_text()))
        
        if publisher_header:
            parent = publisher_header.find_parent('div', class_='a-section') or publisher_header.find_parent('div')
            if parent:
                for link in parent.find_all("a", href=True):
                    match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                    if match and match.group(1) not in asins_trouves:
                        publisher_asins.append(match.group(1))
                        asins_trouves.add(match.group(1))
        
        if publisher_asins:
            logger.info(f"      üè¢ From Publisher: {len(publisher_asins)} volume(s) trouv√©(s)")
        
        result["publisher"] = publisher_asins
    
    # ========================================
    # SECTION 3: Frequently bought together / „Çà„Åè‰∏ÄÁ∑í„Å´Ë≥ºÂÖ•„Åï„Çå„Å¶„ÅÑ„ÇãÂïÜÂìÅ
    # D√©sactiv√© par d√©faut car retourne des hors-sujets
    # Mais utile pour les nouvelles s√©ries ajout√©es manuellement
    # ========================================
    if "frequently_bought" in sources:
        frequently_asins = []
        
        # Chercher la section "Frequently bought together" ou "„Çà„Åè‰∏ÄÁ∑í„Å´Ë≥ºÂÖ•„Åï„Çå„Å¶„ÅÑ„ÇãÂïÜÂìÅ"
        fbt_header = soup.find(lambda tag: tag.name in ['h2', 'h3', 'div', 'span'] and 
                                  ('Frequently bought together' in tag.get_text() or 
                                   '„Çà„Åè‰∏ÄÁ∑í„Å´Ë≥ºÂÖ•„Åï„Çå„Å¶„ÅÑ„ÇãÂïÜÂìÅ' in tag.get_text()))
        
        if fbt_header:
            # Remonter au conteneur parent
            parent = fbt_header.find_parent('div', id='sims-fbt') or \
                     fbt_header.find_parent('div', class_='a-section') or \
                     fbt_header.find_parent('div')
            if parent:
                for link in parent.find_all("a", href=True):
                    match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                    if match and match.group(1) not in asins_trouves:
                        frequently_asins.append(match.group(1))
                        asins_trouves.add(match.group(1))
        
        # Alternative: chercher par ID "sims-fbt"
        if not frequently_asins:
            fbt_section = soup.find('div', id='sims-fbt')
            if fbt_section:
                for link in fbt_section.find_all("a", href=True):
                    match = re.search(r'/dp/([A-Z0-9]{10})', link.get('href', ''))
                    if match and match.group(1) not in asins_trouves:
                        frequently_asins.append(match.group(1))
                        asins_trouves.add(match.group(1))
        
        if frequently_asins:
            logger.info(f"      üõí Frequently bought: {len(frequently_asins)} volume(s) trouv√©(s)")
        
        result["frequently_bought"] = frequently_asins
    
    # Log total
    total = sum(len(v) for v in result.values())
    if total > 0:
        logger.info(f"      ‚úÖ Total: {total} volume(s) potentiel(s) d√©tect√©(s)")
    
    return result


def extraire_volumes_depuis_page_flat(result_dict: Dict[str, List[str]]) -> List[str]:
    """
    Convertit le r√©sultat de extraire_volumes_depuis_page en liste plate d'URLs.
    Utilis√© pour la compatibilit√© avec l'ancien code.
    """
    all_asins = []
    for source in ["bulk", "publisher", "frequently_bought"]:
        all_asins.extend(result_dict.get(source, []))
    
    urls = []
    for asin in all_asins:
        url = f"https://www.amazon.co.jp/dp/{asin}"
        if url not in urls:
            urls.append(url)
    
    return urls


async def extraire_infos_produit(html: str, debug: bool = False) -> Dict:
    """Extrait les infos du produit"""
    if not html:
        return {}
    
    soup = BeautifulSoup(html, 'lxml')
    infos = {}
    
    # D√âTECTION PAGE INVALIDE (captcha, rate limit, etc.)
    # Si pas de titre ET pas de d√©tails produit, c'est probablement une page invalide
    titre = soup.find("span", {"id": "productTitle"})
    titre_texte = titre.get_text(strip=True) if titre else ""
    
    if not titre_texte:
        # V√©rifier si c'est un captcha ou une page d'erreur
        if 'captcha' in html.lower() or 'robot' in html.lower():
            infos['_page_invalide'] = 'captcha'
        elif 'To discuss automated access' in html:
            infos['_page_invalide'] = 'rate_limit'
        elif len(html) < 5000:  # Page trop courte = probablement erreur
            infos['_page_invalide'] = 'page_courte'
        else:
            infos['_page_invalide'] = 'titre_non_trouve'
    
    infos['titre'] = titre_texte  # Sauvegarder le titre pour debug
    
    # DEBUG: Afficher le titre brut si demand√©
    if debug and titre_texte:
        logger.info(f"      [DEBUG] Titre brut: '{titre_texte}'")
    
    # D√©tecter si c'est un lot/set
    if 'Â∑ª„Çª„ÉÉ„Éà' in titre_texte or ('„Çª„ÉÉ„Éà' in titre_texte and ('1-' in titre_texte or 'ÂÖ®Â∑ª' in titre_texte)):
        infos['est_lot'] = True
        # Essayer d'extraire la plage (ex: "1-3Â∑ª" ou "ÂÖ®5Â∑ª")
        match_plage = re.search(r'(\d+)-(\d+)Â∑ª', titre_texte)
        if match_plage:
            infos['lot_debut'] = int(match_plage.group(1))
            infos['lot_fin'] = int(match_plage.group(2))
        elif 'ÂÖ®' in titre_texte:
            match_total = re.search(r'ÂÖ®(\d+)Â∑ª', titre_texte)
            if match_total:
                infos['lot_total'] = int(match_total.group(1))
    else:
        infos['est_lot'] = False
    
    # Date et √âditeur
    # Note : Amazon peut servir la page en japonais (Áô∫Â£≤Êó•/Âá∫ÁâàÁ§æ) ou en anglais
    # (Publication date/Publisher) selon les cookies de session. On cherche les deux.
    details = soup.find("div", {"id": "detailBulletsWrapper_feature_div"})
    if details:
        for li in details.find_all("li"):
            text = li.get_text()
            if "Áô∫Â£≤Êó•" in text or "Publication date" in text:
                date_texte = text.split(":")[-1].strip()
                # Nettoyer les caract√®res Unicode invisibles (U+200E, U+200F, etc.)
                date_texte = re.sub(r'[\u200e\u200f\u200b\u202a\u202b\u202c\xa0]', '', date_texte).strip()
                infos['date'] = date_texte
            elif "Âá∫ÁâàÁ§æ" in text or "Publisher" in text:
                editeur_texte = text.split(":")[-1].strip()
                # Nettoyer les caract√®res Unicode invisibles
                editeur_texte = re.sub(r'[\u200e\u200f\u200b\u202a\u202b\u202c\xa0]', '', editeur_texte).strip()
                # Nettoyer l'√©diteur (enlever la date entre parenth√®ses)
                editeur_brut = re.split(r'\s*\(', editeur_texte)[0].strip()
                # Convertir en romaji
                infos['editeur'] = convertir_editeur_romaji(editeur_brut)
                if debug:
                    logger.info(f"      [DEBUG] √âditeur trouv√©: {editeur_brut} ‚Üí {infos['editeur']}")
    
    # Tome ‚Äî appel √† la fonction unique de parsing (utils.py)
    # Seulement si ce n'est pas un lot
    if not infos.get('est_lot') and titre:
        tome = extraire_numero_tome(titre_texte)
        if tome is not None:
            infos['tome'] = tome
            if debug:
                logger.info(f"      [DEBUG] Tome trouv√©: {tome}")
        elif debug:
            logger.warning(f"      [DEBUG] ‚ö†Ô∏è  AUCUN TOME trouv√© dans: '{titre_texte}'")
    
    # Couverture
    image = soup.find("img", {"id": "landingImage"})
    if image and image.get('src'):
        infos['couverture_url'] = image['src']
    
    # FORMAT DU LIVRE (ÂçòË°åÊú¨, ÊñáÂ∫´, „Éö„Éº„Éë„Éº„Éê„ÉÉ„ÇØ, KindleÁâà, etc.)
    # M√©thode 1: Chercher dans le s√©lecteur de format (tmmSwatches)
    format_section = soup.find("div", {"id": "tmmSwatches"})
    if format_section:
        selected = format_section.find("span", class_="a-button-selected") or format_section.find("li", class_="selected")
        if selected:
            format_text = selected.get_text(strip=True)
            infos['format'] = format_text
            if debug:
                logger.info(f"      [DEBUG] Format trouv√© (tmmSwatches): {format_text}")
    
    # M√©thode 2: Chercher dans les d√©tails du produit
    if 'format' not in infos or not infos['format']:
        detail_bullets = soup.find("div", {"id": "detailBullets_feature_div"})
        if detail_bullets:
            for li in detail_bullets.find_all("li"):
                text = li.get_text()
                # Chercher les mots-cl√©s de format
                if any(f in text for f in ['ÂçòË°åÊú¨', 'ÊñáÂ∫´', '„Éö„Éº„Éë„Éº„Éê„ÉÉ„ÇØ', '„Ç≥„Éü„ÉÉ„ÇØ', 'Paperback', 'Tankobon']):
                    infos['format'] = text.strip()[:50]
                    if debug:
                        logger.info(f"      [DEBUG] Format trouv√© (d√©tails): {infos['format']}")
                    break
    
    # M√©thode 3: Chercher dans le titre lui-m√™me (souvent entre parenth√®ses)
    if 'format' not in infos or not infos['format']:
        # Patterns courants: (ËßíÂ∑ù„Çπ„Éã„Éº„Ç´„ÉºÊñáÂ∫´), (ÂØåÂ£´Ë¶ãLÊñáÂ∫´), („Ç≥„Éü„ÉÉ„ÇØ„Çπ), etc.
        if 'ÊñáÂ∫´' in titre_texte:
            infos['format'] = 'ÊñáÂ∫´'
            if debug:
                logger.info(f"      [DEBUG] Format trouv√© (titre): ÊñáÂ∫´")
        elif '„Ç≥„Éü„ÉÉ„ÇØ' in titre_texte:
            infos['format'] = '„Ç≥„Éü„ÉÉ„ÇØ'
            if debug:
                logger.info(f"      [DEBUG] Format trouv√© (titre): „Ç≥„Éü„ÉÉ„ÇØ")
    
    # M√©thode 4: Chercher le breadcrumb ou la cat√©gorie
    if 'format' not in infos or not infos['format']:
        breadcrumb = soup.find("div", {"id": "wayfinding-breadcrumbs_feature_div"})
        if breadcrumb:
            bc_text = breadcrumb.get_text()
            if 'ÊñáÂ∫´' in bc_text:
                infos['format'] = 'ÊñáÂ∫´'
            elif '„Ç≥„Éü„ÉÉ„ÇØ' in bc_text or '„Éû„É≥„Ç¨' in bc_text:
                infos['format'] = '„Ç≥„Éü„ÉÉ„ÇØ'
            elif 'ÂçòË°åÊú¨' in bc_text:
                infos['format'] = 'ÂçòË°åÊú¨'
    
    return infos


def extraire_item_amazon(item):
    """Extrait titre, lien, URL et ASIN d'un √©l√©ment r√©sultat Amazon.
    Retourne (titre_txt, url_complete, asin) ou (None, None, None) si invalide."""
    titre_elem = item.select_one('.a-text-normal') or item.select_one('h2 a span')
    if not titre_elem:
        return None, None, None
    
    lien_elem = item.select_one('.a-link-normal') or item.select_one('h2 a')
    if not lien_elem or not lien_elem.get('href'):
        return None, None, None
    
    url_complete = f"https://www.amazon.co.jp{lien_elem['href']}"
    titre_txt = titre_elem.get_text()
    asin = extraire_asin(url_complete)
    
    # Aussi essayer data-asin du parent
    if (not asin or asin == '?') and item.get('data-asin'):
        asin = item['data-asin']
    
    return titre_txt, url_complete, asin


def extraire_infos_featured(item, titre_txt: str) -> Dict:
    """
    Extrait les m√©tadonn√©es d'un r√©sultat de recherche Amazon Featured.
    Retourne un dict avec les infos disponibles (titre, date, editeur, tome, format).
    Ces infos permettent de remplir le cache SANS fetcher la page /dp/.
    """
    infos = {'titre': titre_txt}
    
    # Extraire le num√©ro de tome depuis le titre
    tome = extraire_numero_tome(titre_txt)
    if tome is not None:
        infos['tome'] = tome
    
    # Extraire l'√©diteur depuis le titre (entre parenth√®ses japonaises ou normales)
    editeur_titre = extraire_editeur(titre_txt)
    if editeur_titre:
        infos['editeur'] = convertir_editeur_romaji(editeur_titre)
    
    # Extraire la date de publication depuis les spans sous le titre
    # Pattern Amazon JP : "„Ç≥„Éü„ÉÉ„ÇØ ‚Äì 2026/1/23" ou "ÊñáÂ∫´ ‚Äì 2024/8/30"
    for span in item.select('span.a-text-normal, span.a-size-base, span.a-color-secondary'):
        text = span.get_text(strip=True)
        # Chercher un pattern date YYYY/M/D ou YYYY/MM/DD
        date_match = re.search(r'(\d{4}/\d{1,2}/\d{1,2})', text)
        if date_match:
            infos['date'] = date_match.group(1)
            # Le format est souvent juste avant la date : "„Ç≥„Éü„ÉÉ„ÇØ ‚Äì 2026/1/23"
            format_match = re.search(r'(„Ç≥„Éü„ÉÉ„ÇØ|ÊñáÂ∫´|ÂçòË°åÊú¨|Êñ∞Êõ∏|Â§ßÂûãÊú¨|„É†„ÉÉ„ÇØ)', text)
            if format_match:
                infos['format'] = format_match.group(1)
            break
    
    # Aussi chercher la date dans les sous-divs (parfois dans une structure diff√©rente)
    if 'date' not in infos:
        for div in item.select('.a-row'):
            div_text = div.get_text(strip=True)
            date_match = re.search(r'(\d{4}/\d{1,2}/\d{1,2})', div_text)
            if date_match:
                infos['date'] = date_match.group(1)
                break
    
    # Normaliser la date au format YYYY/MM/DD
    if 'date' in infos:
        parts = infos['date'].split('/')
        if len(parts) == 3:
            infos['date'] = f"{parts[0]}/{int(parts[1]):02d}/{int(parts[2]):02d}"
    
    return infos


