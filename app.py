#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MangaVega Tracker - Point d'entr√©e principal (orchestrateur)
"""

import asyncio
import json
import os
import re
import random
import sys
import traceback as tb
from datetime import datetime
from typing import Dict

import config
from database import DatabaseManager
import utils
import sync
import notifications
import pipeline
from scraper import SessionWrapper

logger = config.logger


async def main():
    import argparse
    import traceback as tb
    
    # === ARGUMENTS CLI ===
    parser = argparse.ArgumentParser(description=f"MangaVega Tracker v{config.VERSION}")
    parser.add_argument('--serie', type=str, help='Scanner uniquement les s√©ries contenant ce texte (match partiel)')
    parser.add_argument('--list', action='store_true', help='Afficher le contenu de la BDD et quitter')
    parser.add_argument('--no-push', action='store_true', help='Ne pas faire git push √† la fin')
    parser.add_argument('--no-email', action='store_true', help='Ne pas envoyer les emails')
    parser.add_argument('--reverifier-traductions', action='store_true', help='Re-v√©rifier les traductions non-officielles')
    args = parser.parse_args()
    
    # Mode re-v√©rification traductions
    if args.reverifier_traductions:
        logger.info("\n" + "="*80)
        logger.info(f"üîÑ MODE RE-V√âRIFICATION TRADUCTIONS")
        logger.info("="*80)
        db = DatabaseManager()
        await pipeline.reverifier_toutes_traductions(db)
        return
    
    # Mode liste BDD
    if args.list:
        db = DatabaseManager()
        db.init_table_volumes()
        db.init_table_editeurs()
        conn = db._get_conn()
        cursor = conn.cursor()
        
        # Stats g√©n√©rales
        cursor.execute("SELECT COUNT(*) FROM volumes")
        nb_volumes = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(DISTINCT serie_jp) FROM volumes")
        nb_series = cursor.fetchone()[0]
        
        logger.info(f"\nüìä BASE DE DONN√âES: {nb_volumes} volumes, {nb_series} s√©ries\n")
        
        # Volumes par s√©rie
        cursor.execute("""
            SELECT serie_jp, serie_fr, COUNT(*) as nb, 
                   MIN(tome) as t_min, MAX(tome) as t_max,
                   MIN(date_sortie_jp) as date_min, MAX(date_sortie_jp) as date_max,
                   editeur
            FROM volumes 
            GROUP BY serie_jp 
            ORDER BY serie_jp
        """)
        for row in cursor.fetchall():
            serie_jp, serie_fr, nb, t_min, t_max, date_min, date_max, editeur = row
            nom_display = serie_fr if serie_fr else serie_jp
            logger.info(f"  üìö {nom_display}")
            logger.info(f"     {nb} tome(s) | T{t_min}-T{t_max} | {date_min} ‚Üí {date_max} | {editeur or '?'}")
        
        db.close()
        return
    
    try:
        await _main_inner(args)
    except Exception as e:
        logger.error(f"\n‚ùå ERREUR FATALE: {e}")
        logger.error(tb.format_exc())
        # Ne PAS faire raise : la BDD et les fichiers g√©n√©r√©s doivent √™tre commit√©es
        # Le workflow doit pouvoir faire git add m√™me apr√®s une erreur partielle
        logger.info("‚ö†Ô∏è  Le script a rencontr√© une erreur mais les donn√©es partielles sont conserv√©es")


async def _main_inner(args):
    # NOUVEAU: Charger la liste des mangas depuis le fichier JSON externe
    sync.charger_mangas_liste()
    
    # === FILTRE --serie ===
    if args.serie:
        filtre_texte = args.serie.lower()
        avant = len(config.MANGAS_A_SUIVRE)
        config.MANGAS_A_SUIVRE = [
            m for m in config.MANGAS_A_SUIVRE 
            if filtre_texte in m['nom'].lower() 
            or filtre_texte in config.TRADUCTIONS_FR.get(m['nom'], '').lower()
        ]
        apres = len(config.MANGAS_A_SUIVRE)
        if apres == 0:
            logger.error(f"‚ùå Aucune s√©rie ne correspond au filtre '{args.serie}'")
            logger.info(f"   (sur {avant} s√©ries disponibles)")
            return
        logger.info(f"üîç Filtre --serie '{args.serie}': {apres}/{avant} s√©rie(s) s√©lectionn√©e(s)")
        for m in config.MANGAS_A_SUIVRE:
            logger.info(f"   ‚Üí {m['nom']}")
    
    logger.info("\n" + "="*80)
    logger.info(f"üöÄ MANGA TRACKER v{config.VERSION} ({config.VERSION_DATE})")
    logger.info("="*80)
    logger.info(f"üìö {len(config.MANGAS_A_SUIVRE)} mangas √† surveiller")
    logger.info(f"üìÖ Date seuil nouveaut√©s: {config.DATE_SEUIL.strftime('%Y/%m/%d')}")
    logger.info("="*80)
    
    debut = datetime.now()
    db = DatabaseManager()
    
    # INITIALISATION : Cr√©er/v√©rifier les tables volumes et editeurs
    logger.info("\nüì¶ Initialisation de la base de donn√©es...")
    db.init_table_volumes()
    db.init_table_editeurs()
    logger.info("   ‚úÖ Tables 'volumes' et 'series_editeurs' pr√™tes")
    # === NETTOYAGE : supprimer les doublons de traductions migration_v7 ===
    # Les traductions 'migration_v7' (sans suffixe [MANGA]/[LN]) sont redondantes
    # car rechercher_traductions() ins√®re avec le bon nom (avec suffixe) source='manuel'
    try:
        conn = db._get_conn()
        cursor = conn.cursor()
        cursor.execute("DELETE FROM traductions WHERE source = 'migration_v7'")
        nb_supprimees = cursor.rowcount
        if nb_supprimees > 0:
            conn.commit()
            logger.info(f"   üóëÔ∏è  {nb_supprimees} traduction(s) legacy 'migration_v7' supprim√©es (doublons)")
        cursor.execute("SELECT COUNT(*) FROM traductions")
        nb_trad = cursor.fetchone()[0]
        logger.info(f"   ‚úÖ Traductions manuelles d√©j√† en BDD ({nb_trad} entr√©es)")
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è  Nettoyage traductions: {e}")

    # === MIGRATION : ebooks_traites ‚Üí featured_history (puis suppression table legacy) ===
    try:
        nb_migres = db.migrer_ebooks_vers_featured_history()
        if nb_migres > 0:
            logger.info(f"   ‚úÖ {nb_migres} ebook(s) migr√©s vers featured_history")
        # Supprimer la table legacy apr√®s migration r√©ussie
        try:
            conn = db._get_conn()
            conn.execute('DROP TABLE IF EXISTS ebooks_traites')
            conn.commit()
            logger.info("   üóëÔ∏è  Table legacy 'ebooks_traites' supprim√©e")
        except Exception:
            pass  # Pas grave si elle n'existe d√©j√† plus
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è  Migration featured_history: {e}")
    
    # Charger la configuration depuis le Gist GitHub (viewer sync)
    sync.charger_gist_config()
    
    # Charger les corrections manuelles (depuis Gist + BDD + fichier JSON)
    sync.charger_corrections(db)
    
    # Charger la configuration des s√©ries (depuis Gist + fichier local)
    sync.charger_series_config(db)
    
    # Afficher le nombre de s√©ries apr√®s fusion
    logger.info(f"üìö {len(config.MANGAS_A_SUIVRE)} mangas √† surveiller (apr√®s fusion)")
    
    # V√âRIFICATION : Afficher les s√©ries sans traduction FR
    series_manquantes = db.get_series_sans_traduction()
    # Exclure celles qui ont un titre dans config.TRADUCTIONS_FR (charg√© depuis mangas_liste.json)
    series_manquantes = [s for s in series_manquantes 
                         if not config.TRADUCTIONS_FR.get(s['serie_jp'], '')
                         and not config.TRADUCTIONS_FR.get(utils.strip_type_suffix(s['serie_jp']), '')]
    if series_manquantes:
        nb_sans_fr = len(series_manquantes)
        if nb_sans_fr > 0:
            logger.info(f"   ‚ö†Ô∏è  {nb_sans_fr} s√©rie(s) sans titre FR")
            logger.info(f"   ‚Üí Les traductions seront recherch√©es automatiquement")
    
    logger.info("="*80)
    
    toutes_nouveautes = []
    tous_papiers = []  # NOUVEAU
    
    # OPTIMISATION: Trier les s√©ries par priorit√©
    # INVERS√â: Les s√©ries avec cache passent en premier pour "d√©bloquer" le rate limit Amazon
    # Priorit√© 1: Avec cache ‚Üí passe en premier, survit au rate limit initial
    # Priorit√© 2: ASIN de r√©f√©rence + pas de cache ‚Üí Bulk direct possible
    # Priorit√© 3: Pas de cache ni r√©f√©rence ‚Üí passe √† la fin quand Amazon s'est calm√©
    
    def get_priorite_serie(manga: Dict) -> tuple:
        """
        Retourne un tuple (priorit√©, -nb_cache) pour le tri.
        Plus le tuple est petit, plus la s√©rie est prioritaire.
        """
        nom = manga['nom']
        urls_supp = manga.get('urls_supplementaires', [])
        
        # Compter le cache
        try:
            nb_cache = len(db.get_volumes_connus(nom))
        except:
            nb_cache = 0
        
        # Chercher un ASIN de r√©f√©rence
        asin_ref = None
        
        # Source 0: ASIN de r√©f√©rence explicite (depuis Gist)
        if manga.get('asin_reference'):
            asin_ref = manga['asin_reference']
        
        # Source 1: URL suppl√©mentaire (ajout√©e manuellement)
        if not asin_ref and urls_supp:
            for url in urls_supp:
                match = re.search(r'/dp/([A-Z0-9]{10})', url)
                if match:
                    asin_ref = match.group(1)
                    break
        
        # Source 2: Volume valid√© en BDD
        if not asin_ref:
            asin_ref = db.get_asin_reference(nom)
        
        # D√©terminer la priorit√© (INVERS√â par rapport √† avant)
        if nb_cache > 0:
            priorite = 1  # ü•á Avec cache ‚Üí passe en premier
        elif asin_ref:
            priorite = 2  # ü•à ASIN de r√©f√©rence + pas de cache
        else:
            priorite = 3  # ü•â Pas de cache ni r√©f√©rence ‚Üí √† la fin
        
        # Stocker l'ASIN de r√©f√©rence pour usage ult√©rieur
        manga['_asin_reference'] = asin_ref
        
        return (priorite, -nb_cache)  # -nb_cache pour trier par cache d√©croissant
    
    # Trier les s√©ries
    mangas_tries = sorted(config.MANGAS_A_SUIVRE, key=get_priorite_serie)
    
    # Logger l'ordre (utiliser les valeurs d√©j√† calcul√©es dans _asin_reference)
    p1 = 0
    p2 = 0
    p3 = 0
    for m in mangas_tries:
        nom = m['nom']
        try:
            nb_cache = len(db.get_volumes_connus(nom))
        except:
            nb_cache = 0
        asin_ref = m.get('_asin_reference')
        
        if nb_cache > 0:
            p1 += 1
        elif asin_ref:
            p2 += 1
        else:
            p3 += 1
    
    logger.info(f"üìã Ordre optimis√© (cache en premier):")
    logger.info(f"   ü•á {p1} s√©rie(s) avec cache (passent d'abord)")
    logger.info(f"   ü•à {p2} s√©rie(s) avec ASIN de r√©f√©rence, sans cache")  
    logger.info(f"   ü•â {p3} s√©rie(s) sans cache ni r√©f√©rence (√† la fin)")
    logger.info("")
    
    # NOTE: Le d√©lai initial de 5 minutes a √©t√© test√© mais n'aide pas
    # Le rate limit Amazon semble bas√© sur l'IP, pas sur le timing
    # On garde juste un petit d√©lai de 10s pour "chauffer" la connexion
    import os
    if os.environ.get('GITHUB_ACTIONS'):
        logger.info(f"‚è≥ GitHub Actions d√©tect√© - Petit d√©lai de 10s avant le scan...")
        await asyncio.sleep(10)
    
    # S√âQUENTIEL (un par un) avec d√©lais anti-rate-limit
    async with SessionWrapper() as session:
        # Warm-up : visiter amazon.co.jp pour obtenir les cookies de session
        await session.warm_up()
        
        series_echouees = []  # S√©ries avec 0 r√©sultat (probable 503)
        
        async def scanner_serie(manga, index, total, est_retry=False):
            """Scanne une s√©rie et retourne (nouveautes, papiers).
            Retourne None si la s√©rie est bloqu√©e (0 r√©sultat)."""
            prefix = "üîÑ RETRY" if est_retry else "üìö MANGA"
            
            filtre = manga.get('filtre')
            serie_id = manga.get('serie_id')
            urls_supplementaires = manga.get('urls_supplementaires', [])
            asin_reference = manga.get('_asin_reference')
            
            nouveautes, papiers = await pipeline.rechercher_manga(
                session, db, 
                manga['nom'], 
                manga['url_suffix'],
                filtre=filtre,
                serie_id=serie_id,
                asin_reference=asin_reference,
                urls_supplementaires=urls_supplementaires if urls_supplementaires else None
            )
            
            # Nettoyage URLs suppl√©mentaires du Gist
            if urls_supplementaires:
                serie_nom = manga['nom']
                if config.GIST_SERIES_CONFIG.get('urls_supplementaires', {}).get(serie_nom):
                    del config.GIST_SERIES_CONFIG['urls_supplementaires'][serie_nom]
                    config.GIST_MODIFIED = True
                    logger.info(f"   üßπ URL(s) suppl√©mentaire(s) retir√©e(s) du Gist")
            
            # Recherche √©tendue des tomes manquants via Bulk
            analyse_tomes = utils.analyser_tomes_manquants(papiers)
            if not analyse_tomes['complet'] and len(analyse_tomes['tomes_manquants']) > 0:
                if len(analyse_tomes['tomes_manquants']) <= 5:
                    logger.info(f"\n   ‚ö†Ô∏è Tomes manquants d√©tect√©s: {sorted(analyse_tomes['tomes_manquants'])} (sur {analyse_tomes['tome_max']} attendus)")
                    asins_deja_connus = {p['asin'] for p in papiers if p.get('asin')}
                    nouveaux_trouves = await pipeline.rechercher_volumes_via_bulk_etendu(
                        session, db, manga['nom'], papiers, 
                        asins_deja_connus, config.ASINS_HORS_SUJET, logger
                    )
                    if nouveaux_trouves:
                        logger.info(f"   üéâ {len(nouveaux_trouves)} nouveau(x) volume(s) trouv√©(s) via Bulk √©tendu !")
                        papiers.extend(nouveaux_trouves)
                else:
                    logger.info(f"\n   ‚ö†Ô∏è {len(analyse_tomes['tomes_manquants'])} tomes manquants")
            
            return nouveautes, papiers
        
        # === BOUCLE PRINCIPALE ===
        for i, manga in enumerate(mangas_tries, 1):
          try:
            nouveautes, papiers = await scanner_serie(manga, i, len(mangas_tries))
            toutes_nouveautes.extend(nouveautes)
            tous_papiers.extend(papiers)
            
            # D√©tecter les s√©ries bloqu√©es (0 r√©sultat)
            if len(papiers) == 0:
                series_echouees.append(manga)
            
            # Pause √† mi-parcours pour √©viter les 503
            if i == 28:  # √Ä la moiti√© des 55 mangas
                logger.info("\n" + "="*80)
                logger.info("‚è∏Ô∏è  PAUSE LONGUE DE 60 SECONDES √Ä MI-PARCOURS")
                logger.info("    (Pour √©viter le rate limiting Amazon)")
                logger.info("="*80 + "\n")
                await asyncio.sleep(60)
            
            # D√©lai adaptatif entre mangas
            if i < len(mangas_tries):
                serie_bloquee = (len(papiers) == 0 and len(nouveautes) == 0)
                
                if i % 15 == 0:  # Pause tous les 15 mangas
                    logger.info(f"\n‚è∏Ô∏è  Pause de 8s apr√®s {i} mangas pour √©viter le rate limiting...")
                    await asyncio.sleep(8)
                elif serie_bloquee:
                    logger.info(f"   ‚è∏Ô∏è  Pause de 15s (r√©cup√©ration apr√®s blocage)...")
                    await asyncio.sleep(15)
                else:
                    await asyncio.sleep(random.uniform(1.5, 3))
          
          except Exception as e:
            logger.error(f"‚ùå ERREUR pour {manga.get('nom', '?')}: {e}")
            import traceback
            logger.error(traceback.format_exc())
            series_echouees.append(manga)
            continue
        
        # === RETRY DES S√âRIES √âCHOU√âES ===
        # √Ä ce stade les cookies sont √©tablis, les retries ont de bonnes chances de passer
        if series_echouees:
            logger.info("\n" + "="*80)
            logger.info(f"üîÑ RETRY: {len(series_echouees)} s√©rie(s) sans r√©sultat lors du premier passage")
            logger.info("   (Les cookies sont maintenant √©tablis, retry apr√®s 30s de pause)")
            logger.info("="*80)
            await asyncio.sleep(30)
            
            nb_recuperees = 0
            for j, manga in enumerate(series_echouees, 1):
              try:
                logger.info(f"\nüîÑ [{j}/{len(series_echouees)}] Retry: {manga['nom']}")
                nouveautes, papiers = await scanner_serie(manga, j, len(series_echouees), est_retry=True)
                toutes_nouveautes.extend(nouveautes)
                tous_papiers.extend(papiers)
                
                if len(papiers) > 0:
                    nb_recuperees += 1
                    logger.info(f"   ‚úÖ R√©cup√©r√©e: {len(papiers)} volume(s)")
                else:
                    logger.warning(f"   ‚ùå Toujours aucun r√©sultat")
                
                # D√©lai entre retries
                if j < len(series_echouees):
                    await asyncio.sleep(random.uniform(3, 6))
              
              except Exception as e:
                logger.error(f"‚ùå ERREUR retry {manga.get('nom', '?')}: {e}")
                continue
            
            logger.info(f"\nüîÑ Retry termin√©: {nb_recuperees}/{len(series_echouees)} s√©rie(s) r√©cup√©r√©e(s)")
        
        # === CORRECTION DES TOMES MANQUANTS ===
        # Recherche les num√©ros de tome pour les volumes valid√©s manuellement 
        # qui ont un tome = ? ou N/A (souvent des URLs ajout√©es manuellement)
        tomes_corriges = await pipeline.corriger_tomes_manquants(session, db, logger)
    
    fin = datetime.now()
    duree = (fin - debut).total_seconds()
    
    logger.info("\n" + "="*80)
    logger.info("üìä R√âSUM√â FINAL")
    logger.info("="*80)
    logger.info(f"‚è±Ô∏è  Temps: {duree:.1f}s")
    logger.info(f"üìö Scann√©s: {len(config.MANGAS_A_SUIVRE)}")
    logger.info(f"üì¶ Papiers trouv√©s: {len(tous_papiers)}")
    logger.info(f"‚ú® Nouveaut√©s: {len(toutes_nouveautes)}")
    logger.info("="*80)
    
    # G√©n√©rer le r√©sum√© par s√©rie dans le log
    if tous_papiers:
        pipeline.generer_resume_log(tous_papiers, logger)
    
    logger.info("\n" + "="*80)
    logger.info("üìÅ FICHIERS G√âN√âR√âS")
    logger.info("="*80)
    
    # Export JSON avec statuts (pour le viewer)
    nb_non_traites = 0
    if tous_papiers:
        asins_rejetes = db.get_asins_rejetes()
        asins_valides = db.get_asins_valides()
        volume_overrides = db.get_all_volume_serie_overrides()
        
        # Enrichir les volumes avec leur statut et nom_fr
        volumes_avec_statut = []
        for p in tous_papiers:
            p_copy = p.copy()
            asin = p.get('asin', '')
            nom = p.get('nom', '')
            
            # NOUVEAU: Appliquer l'override de s√©rie si d√©fini
            if asin in volume_overrides:
                serie_alternative = volume_overrides[asin]
                p_copy['nom'] = serie_alternative  # Remplacer le nom de s√©rie
                p_copy['nom_fr'] = serie_alternative  # La s√©rie alternative est d√©j√† en FR
                p_copy['serie_originale'] = nom  # Garder trace de l'origine
            else:
                # Ajouter nom_fr depuis le dictionnaire de traductions
                if 'nom_fr' not in p_copy or not p_copy.get('nom_fr'):
                    p_copy['nom_fr'] = config.TRADUCTIONS_FR.get(nom, '')
            
            # NOUVEAU: Ajouter la date de premi√®re d√©tection
            # On utilise la date d'aujourd'hui (sera conserv√©e lors des fusions avec l'historique)
            p_copy['date_detection'] = datetime.now().strftime('%Y-%m-%d')
            
            # Ajouter le statut
            if asin in asins_rejetes:
                p_copy['statut'] = 'rejete'
            elif asin in asins_valides:
                p_copy['statut'] = 'valide'
            else:
                p_copy['statut'] = 'non_traite'
            volumes_avec_statut.append(p_copy)
        
        # Calculer les stats bas√©es uniquement sur les ASINs pr√©sents dans ce run
        asins_papiers = {p.get('asin', '') for p in tous_papiers}
        nb_valides = len(asins_valides & asins_papiers)
        nb_rejetes = len(asins_rejetes & asins_papiers)
        nb_non_traites = len(asins_papiers) - nb_valides - nb_rejetes
        
        json_data = {
            "generated_at": datetime.now().isoformat(),
            "version": config.VERSION,
            "total_volumes": len(tous_papiers),
            "total_series": len(set(p['nom'] for p in tous_papiers)),
            "stats": {
                "valides": nb_valides,
                "rejetes": nb_rejetes,
                "non_traites": nb_non_traites
            },
            "volumes": volumes_avec_statut
        }
        with open('manga_collection.json', 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        logger.info("üìã JSON collection: manga_collection.json")
    
    # === SAUVEGARDE DU GIST (nettoyage URLs trait√©es) ===
    try:
        sync.sauvegarder_gist_config()
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Erreur sauvegarde Gist (non-bloquant): {e}")
    
    if args.no_email:
        logger.info("üìß Emails d√©sactiv√©s (--no-email)")
    elif toutes_nouveautes:
        logger.info("\n")
        try:
            notifications.envoyer_email(config.EMAIL_DESTINATAIRE, toutes_nouveautes)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur envoi email nouveaut√©s (non-bloquant): {e}")
    
    # Toujours envoyer un rapport de synth√®se (sauf --no-email)
    if not args.no_email:
        try:
            notifications.envoyer_email_rapport(config.EMAIL_DESTINATAIRE, len(config.MANGAS_A_SUIVRE), len(tous_papiers), len(toutes_nouveautes), nb_non_traites, duree)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur envoi rapport (non-bloquant): {e}")
    
    # Fermer la connexion BDD
    db.close()
    
    # Git push (sauf --no-push)
    if args.no_push:
        logger.info("üì§ Git push d√©sactiv√© (--no-push)")
    else:
        try:
            sync.git_push()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur git push (non-bloquant): {e}")


if __name__ == "__main__":
    asyncio.run(main())
